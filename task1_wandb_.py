# -*- coding: utf-8 -*-
"""task1_wandb_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11ouB1HwGF7fTUwXO8-0z6LZETce7X9oe
"""

!pip install wandb

!pip install optuna

import wandb
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    balanced_accuracy_score,
    precision_score,
    recall_score,
    confusion_matrix,
)
import optuna


def plot_confusion_matrix(cm):
    plt.figure(figsize=(9, 9))
    plt.imshow(cm, interpolation="nearest", cmap="Pastel1")
    plt.title("Confusion matrix", size=15)
    plt.colorbar()
    tick_marks = np.arange(10)
    plt.xticks(tick_marks, ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"], rotation=45, size=10)
    plt.yticks(tick_marks, ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"], size=10)
    plt.tight_layout()
    plt.ylabel("Actual label", size=15)
    plt.xlabel("Predicted label", size=15)
    width, height = cm.shape
    for x in range(width):
        for y in range(height):
            plt.annotate(str(cm[x][y]), xy=(y, x), horizontalalignment="center", verticalalignment="center")
    wandb.log({"Confusion Matrix Plot": wandb.Image(plt)})
    plt.close()

def get_hyper_params_from_optuna(trial):
    # Define hyperparameters
    penalty = trial.suggest_categorical("penalty", ["l1", "l2", "elasticnet", "none"])

    if penalty == "none":
        solver_choices = ["newton-cg", "lbfgs", "sag", "saga"]
    elif penalty == "l1":
        solver = "liblinear"
    elif penalty == "l2":
        solver_choices = ["newton-cg", "lbfgs", "liblinear", "sag", "saga"]
    elif penalty == "elasticnet":
        solver = "saga"

    if not (penalty == "l1" or penalty == "elasticnet"):
        solver = trial.suggest_categorical("solver_" + penalty, solver_choices)

    C = trial.suggest_float("inverse_of_regularization_strength", 0.1, 1)

    fit_intercept = trial.suggest_categorical("fit_intercept", [True, False])

    intercept_scaling = trial.suggest_float("intercept_scaling", 0.1, 1.0)

    if penalty == "elasticnet":
        l1_ratio = trial.suggest_float("l1_ratio", 0, 1)
    else:
        l1_ratio = None
    wandb.config.update({
        "penalty": penalty,
        "solver": solver,
        "C": C,
        "fit_intercept": fit_intercept,
        "intercept_scaling": intercept_scaling,
        "l1_ratio": l1_ratio
    })

    return penalty, solver, C, fit_intercept, intercept_scaling, l1_ratio


def sanity_checks(digits):
    wandb.log({
    "metadata/Image Data Shape": {"rows": digits.data.shape[0], "columns": digits.data.shape[1]},
    "metadata/Label Data Shape": {"rows": digits.target.shape[0], "columns": 1}
    })
    # Plotting for sanity checks
    plt.figure(figsize=(20, 4))
    for index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):
        plt.subplot(1, 5, index + 1)
        plt.imshow(np.reshape(image, (8, 8)), cmap=plt.cm.gray)
        plt.title(f"Training: {label}", fontsize=20)
    wandb.log({"sanitychecks/Sanity Checks Plot": wandb.Image(plt)})
    plt.close()

def visualize_test(x_test, y_test, predictions):
    # Log individual test predictions as images
    for i, (image, label, prediction) in enumerate(zip(x_test, y_test, predictions)):
        plt.figure()
        plt.imshow(np.reshape(image, (8, 8)), cmap=plt.cm.gray)
        plt.title(f"Label: {label}, Prediction: {prediction}")
        wandb.log({f"visualizetests/Test Prediction {i}": wandb.Image(plt)})
        plt.close()

def evaluate_model(logisticRegr, x_test, y_test, predictions):
    score = logisticRegr.score(x_test, y_test)
    balanced_accuracy = balanced_accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='macro')
    recall = recall_score(y_test, predictions, average='macro')
    metrics = {
        "Mean Accuracy": score,
        "Balanced Accuracy": balanced_accuracy,
        "Precision": precision,
        "Recall": recall
    }
    wandb.log(metrics)
    return metrics

def show_confusion_matrix(y_test, predictions):
    cm = confusion_matrix(y_test, predictions)
    wandb.log({"Confusion Matrix": wandb.Table(data=cm.tolist(), columns=[f"Predicted {i}" for i in range(10)])})
    plot_confusion_matrix(cm)

def objective(trial):
    wandb.init(project="task1_wandb_", entity="anjanaslekshmi", reinit=True)

    digits = datasets.load_digits()

    sanity_checks(digits)

    x_train, x_test, y_train, y_test = train_test_split(
        digits.data, digits.target, test_size=0.25, random_state=0
    )
    penality, solver, C, fit_intercept, intercept_scaling, l1_ratio = get_hyper_params_from_optuna(trial)
    wandb.config.update({
        "penalty": penality,
        "solver": solver,
        "C": C,
        "fit_intercept": fit_intercept,
        "intercept_scaling": intercept_scaling,
        "l1_ratio": l1_ratio
    })

    logisticRegr = LogisticRegression(
        penalty=penality,
        C=C,
        fit_intercept=fit_intercept,
        intercept_scaling=intercept_scaling,
        solver=solver,
        l1_ratio=l1_ratio,
    )
    logisticRegr.fit(x_train, y_train)
    predictions = logisticRegr.predict(x_test)

    visualize_test(x_test, y_test, predictions)

    metrics = evaluate_model(logisticRegr, x_test, y_test, predictions)

    show_confusion_matrix(y_test, predictions)
    wandb.finish()
    return metrics["Balanced Accuracy"]

def main():
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=5)

    trial = study.best_trial

    print(f"Balanced Accuracy: {trial.value}")
    print(f"Best hyperparameters: {trial.params}")

main()

